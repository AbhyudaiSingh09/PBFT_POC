Hereâ€™s your cleaned-up and fully Markdown-compliant README.md â€” all syntax, indentation, and code block formatting fixed:

â¸»


# PBFT Proof of Concept

A lightweight **Byzantine Fault Tolerance (PBFT)** simulation written in **Rust**.  
This project demonstrates how nodes in a distributed system can communicate, broadcast, and handle consensus messages across a small cluster using **Axum**, **Reqwest**, and **tracing**.

---

## ğŸ§  Overview

Each node runs as an independent HTTP server (via **Axum**) that:

- Loads its identity and peer list from a shared `config/cluster.toml`.
- Exposes endpoints for health checks, peer discovery, and PBFT message handling.
- Logs all events to per-node files under the `logs/` directory.
- Demonstrates the PBFT flow with `Proposal`, `Prepare`, and `Commit` messages.

The simulation currently supports:

- âœ… Cluster of 4 nodes (1 potential Byzantine / bad node configurable later)
- âœ… Config-driven peer registry
- âœ… Typed PBFT message handling
- âœ… Broadcasting via HTTP
- âœ… Structured per-node file logging

---

## ğŸ—ï¸ Project Structure

PBFT_POC/
â”œâ”€â”€ Cargo.toml                # Dependencies and build metadata
â”œâ”€â”€ config/
â”‚   â””â”€â”€ cluster.toml          # Cluster configuration (hosts and ports)
â”œâ”€â”€ logs/                     # Log files created at runtime (per node)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ broadcast_proposal.sh # Helper script to test message broadcast
â””â”€â”€ src/
â”œâ”€â”€ main.rs               # Entry point â€“ spawns all nodes
â”œâ”€â”€ config_load.rs        # Loads cluster.toml config
â”œâ”€â”€ logging.rs            # File-based tracing setup per node
â”œâ”€â”€ node.rs               # Node startup & broadcast logic
â”œâ”€â”€ routes.rs             # HTTP handlers
â””â”€â”€ types.rs              # Shared PBFT message and response types

---

## âš™ï¸ Configuration

Edit `config/cluster.toml` to define your cluster:

```toml
[[nodes]]
id = 0
host = "127.0.0.1"
port = 8080

[[nodes]]
id = 1
host = "127.0.0.1"
port = 8081

[[nodes]]
id = 2
host = "127.0.0.1"
port = 8082

[[nodes]]
id = 3
host = "127.0.0.1"
port = 8083

Add or remove nodes here to simulate larger networks.

â¸»

ğŸš€ Run the Simulation

1. Build and start all nodes

RUST_LOG=info cargo run

This spawns 4 nodes locally:

127.0.0.1:8080
127.0.0.1:8081
127.0.0.1:8082
127.0.0.1:8083

2. Check cluster health

curl -s http://127.0.0.1:8080/peers | jq .

3. Send a PBFT Proposal

./scripts/broadcast_proposal.sh

Or manually:

curl -s -X POST http://127.0.0.1:8080/broadcast \
  -H 'content-type: application/json' \
  -d '{"kind":"Proposal","height":1,"proposer":0,"block":{"txs":[{"k":"k1","v":"v1"}]}}' | jq .

Expected Output
	â€¢	Node 0 broadcasts a Proposal.
	â€¢	Nodes 1â€“3 log recv Proposal.
	â€¢	Each honest node rebroadcasts a Prepare.
	â€¢	All nodes log incoming Prepare messages.

â¸»

ğŸ§¾ Logs

Each node writes logs under the logs/ directory:

logs/
â”œâ”€â”€ node-0.log
â”œâ”€â”€ node-1.log
â”œâ”€â”€ node-2.log
â””â”€â”€ node-3.log

Tail any log in real time:

tail -f logs/node-2.log


â¸»

ğŸ” Next Steps (Planned)
	â€¢	Add quorum tracking for Prepare and Commit phases (2f + 1 logic)
	â€¢	Simulate one Byzantine (faulty) node
	â€¢	Add cryptographic signatures or message digests
	â€¢	Implement block execution and commit decision
	â€¢	Add /metrics endpoint to expose PBFT state

â¸»

ğŸ§° Tech Stack

Component	Purpose
Rust + Tokio	Async runtime for concurrent nodes
Axum	Lightweight HTTP server for message endpoints
Reqwest	HTTP client for node-to-node broadcasting
tracing / tracing-subscriber / tracing-appender	Structured, per-node file logging
config	Loads cluster.toml for node topology
serde / serde_json	JSON serialization for messages

ğŸ¤ Author

Abhyudai Singh
ğŸ“§ abhyudaisingh09@gmail.com
ğŸ”— github.com/AbhyudaiSingh09
